{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "kzKUrzGGm-kK"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import make_moons\n",
        "import tensorflow\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = make_moons(n_samples=150, noise=0.15, random_state=42)"
      ],
      "metadata": {
        "id": "uhfVL9SJnBmf"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(X[:,0], X[:,1], c=y); # 2 columns of X data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "8NeGbKSsnFcu",
        "outputId": "a873199f-9bce-4096-bace-69a361575f04"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EFt7hgXQnIgq",
        "outputId": "a7fd7b1a-87b9-4ba3-f4d1-a1988a28c406"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "150"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# adding a bias\n",
        "def add_bias(X):\n",
        "    return np.hstack([X, np.ones((X.shape[0], 1))])"
      ],
      "metadata": {
        "id": "Ru8vLBoqnU6M"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calculating the sigmoid (function returns a NumPy array,where the original values x have been transformed by the sigmoid function\n",
        "def sigmoid(x):\n",
        "    return 1/(1+np.exp(-x))"
      ],
      "metadata": {
        "id": "Wg0auup4nYhr"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test = np.array([-10.0, -1.0, 0.0, 1.0, 10.0])\n",
        "expected = np.array([0.0, 0.27, 0.5, 0.73, 1.0])\n",
        "assert np.all(sigmoid(test).round(2) == expected)"
      ],
      "metadata": {
        "id": "vpAQ1ZISneHg"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  initializing the weights\n",
        "# number of neurons (input, first hidden layer)\n",
        "n_neurons = 2\n",
        "\n",
        "weights = []\n",
        "weights.append(np.random.randn(X.shape[1], n_neurons)) # between input and hidden\n",
        "weights.append(np.random.randn(n_neurons+1, 1)) # between hidden and output\n",
        "\n",
        "weights"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5qcpmRvvnh0c",
        "outputId": "c08981a9-3dc7-4af6-8552-501aa2ab9ea3"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([[-1.20486782,  0.02358108],\n",
              "        [ 1.18640555,  0.7861653 ]]), array([[-0.78846492],\n",
              "        [ 1.20449442],\n",
              "        [-0.07980374]])]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feed-forward network"
      ],
      "metadata": {
        "id": "kER_yi5wn3Ux"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def feed_forward(X, weights):\n",
        "\n",
        "    \"\"\"\n",
        "    1. Multiply the input matrix X (shape of (150, 3))\n",
        "       with the weights of the first layer (shape of (3, 2)),\n",
        "       resulting in the matrix of the shape (150, 2)\n",
        "    \"\"\"\n",
        "    input_layer_1 = np.dot(X, weights[0]) \n",
        "    print(f\"Shape of the first input for layer 1 after dot product: {input_layer_1.shape}\")\n",
        "\n",
        "    \"\"\"    \n",
        "    2. Apply the sigmoid function on the result,\n",
        "       preserving the shape (150, 2) \n",
        "    \"\"\"\n",
        "    output_layer_1 = sigmoid(input_layer_1)\n",
        "    print(f\"Shape of the output of layer 1: {output_layer_1.shape}\")\n",
        "    \n",
        "    \"\"\"    \n",
        "    3. Append an extra column of ones to the result (i.e. the bias),\n",
        "       resulting in the shape (150, 3)\n",
        "    \"\"\"\n",
        "    hidden_1 = add_bias(output_layer_1)\n",
        "    print(f\"Shape of the hidden l1 input with biases: {hidden_1.shape}\")\n",
        "\n",
        "    \"\"\"    \n",
        "    4. Multiply the output of the previous step (shape of (50, 3))\n",
        "       with the weights of the second (i.e. outer) layer (shape of (3, 1)),\n",
        "       resulting in the shape (50, 1)\n",
        "    \"\"\"\n",
        "    input_layer_2 = np.dot(hidden_1, weights[1])\n",
        "    print(f\"Shape of the hidden l1 (overall l2) input weighted sum: {input_layer_2.shape}\")\n",
        "\n",
        "    \"\"\"    \n",
        "    5. Apply the sigmoid function on the result, shape remains (50, 1)\n",
        "    \"\"\"\n",
        "    output_layer_2 = sigmoid(input_layer_2)\n",
        "    \n",
        "    \"\"\"    \n",
        "    6. Return all intermediate results (i.e. anything that\n",
        "       an activation function outputs).\n",
        "    \"\"\"\n",
        "\n",
        "    return output_layer_1, output_layer_2"
      ],
      "metadata": {
        "id": "VrTfiA6un0VO"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_layer_1, output_layer_2 = feed_forward(X, weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0GXRiYGbn1OI",
        "outputId": "b2ef447c-60fc-4f2c-8b33-3bfaf515157a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of the first input for layer 1 after dot product: (150, 2)\n",
            "Shape of the output of layer 1: (150, 2)\n",
            "Shape of the hidden l1 input with biases: (150, 3)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (150, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "f = plt.figure(figsize=(15,6))\n",
        "ax1 = f.add_subplot(121)  # row 1, col 2, index 1\n",
        "ax2 = f.add_subplot(122)\n",
        "x = np.linspace(0,4,1000)\n",
        "ax1.scatter(X[:,0], X[:,1], c = y)\n",
        "ax1.set_title('real classes')\n",
        "\n",
        "ax2.scatter(X[:,0], X[:,1], c = output_layer_2)\n",
        "ax2.set_title('predicted classes with FFN')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "id": "tAzOLKR-pl2F",
        "outputId": "c1bf200a-006d-4bfa-9572-78f7654298fa"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'predicted classes with FFN')"
            ]
          },
          "metadata": {},
          "execution_count": 11
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1080x432 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Backpropagation"
      ],
      "metadata": {
        "id": "ae9O9zHxoNXg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output_layer_1, output_layer_2 = feed_forward(X, weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZWhZgu9-oJhg",
        "outputId": "3fa43353-25bd-4b9c-9b41-615e9e6b6ddd"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of the first input for layer 1 after dot product: (150, 2)\n",
            "Shape of the output of layer 1: (150, 2)\n",
            "Shape of the hidden l1 input with biases: (150, 3)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (150, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid_deriv(x):\n",
        "    return sigmoid(x)*(1-sigmoid(x))"
      ],
      "metadata": {
        "id": "asUwiXSwoRp2"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def log_loss(ytrue, ypred):\n",
        "    ''' return the log loss'''\n",
        "    loss = -(y*np.log(ypred)+(1-y)*np.log(1-ypred)) \n",
        "    return loss"
      ],
      "metadata": {
        "id": "gNOYnf9coXkL"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test\n",
        "y = np.array([0.0, 0.0, 1.0, 1.0])\n",
        "ypred = np.array([0.01, 0.99, 0.01, 0.99])\n",
        "expected = np.array([0.01, 4.61, 4.61, 0.01])\n",
        "assert np.all(log_loss(y, ypred).round(2) == expected)"
      ],
      "metadata": {
        "id": "MP6VeMwKoeSK"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def log_loss_deriv(y, ypred):\n",
        "    loss_deriv = -(y/ypred - (1-y)/(1-ypred))\n",
        "    return loss_deriv"
      ],
      "metadata": {
        "id": "O4l9DLmHohWK"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = np.array([0.5, 0.3, 0.99, 0.2])\n",
        "b = np.array([0.4, 0.2, 0.10, 0.3])\n",
        "expected = np.array([-0.42, -0.62, -9.89, 0.48])\n",
        "assert np.all(log_loss_deriv(a, b).round(2) == expected)"
      ],
      "metadata": {
        "id": "3h_iMuterP1P"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_layer_1, output_layer_2 = feed_forward(X, weights)\n",
        "ytrue = y.reshape(-1, 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HBcAfIOhrssq",
        "outputId": "47bcfdd6-0dbb-444b-ccde-4786010ecaf2"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of the first input for layer 1 after dot product: (150, 2)\n",
            "Shape of the output of layer 1: (150, 2)\n",
            "Shape of the hidden l1 input with biases: (150, 3)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (150, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss = log_loss(ytrue, output_layer_2)"
      ],
      "metadata": {
        "id": "RQ8RS5Gbopvm"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ytrue.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EjqgvdTNortA",
        "outputId": "cb338747-de13-47c9-8aa7-67f6743b1915"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output_layer_2.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eF5DEaSBouYE",
        "outputId": "a9f4b913-e8b7-41c0-8028-3cb47a00bfd0"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(150, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BfcBVul9ow5m",
        "outputId": "eba8fb02-71f8-4c8d-ed1c-c2bd0a75d92a"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(150, 4)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#print(output_layer_2, ytrue, loss)"
      ],
      "metadata": {
        "id": "Zt0_uM6jozNG"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def backprop(weights,\n",
        "             output1,\n",
        "             output2,\n",
        "             ytrue,\n",
        "             X_input,\n",
        "             lr):\n",
        "\n",
        "    wH = weights[0]\n",
        "    wO = weights[1]\n",
        "\n",
        "    '''EQUATION A:'''\n",
        "    error = log_loss_deriv(ytrue, output2)\n",
        "    \n",
        "    '''EQUATION B:'''\n",
        "    #don't forget the bias!\n",
        "    hidden_out_with_bias = add_bias(output1)\n",
        "    #derivative of the sigmoid function with respect to the\n",
        "    #hidden output * weights\n",
        "    sig_deriv_1 = sigmoid_deriv(hidden_out_with_bias.dot(wO))\n",
        "    y_grad = sig_deriv_1 * error\n",
        "\n",
        "    '''EQUATION C:'''\n",
        "    delta_wO = -np.dot(y_grad.T, hidden_out_with_bias) * lr\n",
        "\n",
        "    #and finally, old weights + delta weights -> new weights!\n",
        "    wO_new = wO + delta_wO.T\n",
        "\n",
        "    '''EQUATION D:'''\n",
        "    sig_deriv_2 = sigmoid_deriv(X_input.dot(wH))\n",
        "    #exclude the bias (last column) of the outer weights,\n",
        "    #since it is not backpropagated!\n",
        "    H_grad = sig_deriv_2 * np.dot(y_grad , wO[:-1].T)\n",
        "    \n",
        "    '''EQUATION E:'''\n",
        "    delta_wH = -np.dot(H_grad.T, X_input) * lr\n",
        "    #old weights + delta weights -> new weights!\n",
        "    wH_new = wH + delta_wH.T\n",
        "    \n",
        "    # new hidden weights, new output weights\n",
        "    return wH_new, wO_new"
      ],
      "metadata": {
        "id": "Zes6M2KDo2oX"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr = 0.01 # - change the value for learning rate in backprop to see how your model performance changes\n",
        "n_neurons = 5 # change the number of neurons to see how the performance of NN changes\n",
        "X, y = make_moons(n_samples=200, noise=0.2, random_state=42)\n",
        "X = add_bias(X)\n",
        "y = y.reshape(-1, 1)\n",
        "weights = [\n",
        "   np.random.randn(3, n_neurons),\n",
        "   np.random.randn(n_neurons+1, 1)\n",
        "]\n",
        "\n",
        "# train\n",
        "LOSS_VEC = []\n",
        "\n",
        "for i in range(1000):\n",
        "    out1, out2 = feed_forward(X, weights)\n",
        "    LOSS_VEC.append(log_loss(y, out2).sum())\n",
        "    new_weights = backprop(weights, out1, out2, y, X, lr)\n",
        "    weights = new_weights"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "el6V8VJpo85J",
        "outputId": "f956d59c-1529-42ff-9a23-a25e80f8f172"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
            "Shape of the first input for layer 1 after dot product: (200, 5)\n",
            "Shape of the output of layer 1: (200, 5)\n",
            "Shape of the hidden l1 input with biases: (200, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(LOSS_VEC)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "AfEwdR5apAVb",
        "outputId": "7ebf22b1-4626-4e88-a617-309d6caf796a"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7fbd5f169090>]"
            ]
          },
          "metadata": {},
          "execution_count": 26
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create a grid of values\n",
        "x = np.linspace(-3, 3, 200)\n",
        "X_vis = np.array([(x1, x2) for x1 in x for x2 in x])\n",
        "# add the bias column\n",
        "X_vis = add_bias(X_vis)\n",
        "\n",
        "# calculate the (random) predictions\n",
        "_, y_pred = feed_forward(X_vis, weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B3dly9W4pLQ0",
        "outputId": "4df0a834-c3b2-4de6-b740-7dd2c5315943"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of the first input for layer 1 after dot product: (40000, 5)\n",
            "Shape of the output of layer 1: (40000, 5)\n",
            "Shape of the hidden l1 input with biases: (40000, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (40000, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create the data\n",
        "x = np.linspace(-3, 3, 200)\n",
        "X_vis = np.array([(x1, x2) for x1 in x for x2 in x])\n",
        "X_vis = add_bias(X_vis)\n",
        "_, y_pred = feed_forward(X_vis, weights)\n",
        "Z = y_pred.reshape((len(x), len(x)), order='F')\n",
        "\n",
        "# plotting\n",
        "fig,ax=plt.subplots(1,1)\n",
        "cp = ax.contourf(x, x, Z, alpha=0.8)\n",
        "ax.contour(x, x, Z, levels=[0.5])\n",
        "fig.colorbar(cp) # Add a colorbar to a plot\n",
        "ax.scatter(X[:,0], X[:,1], c=y);"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "id": "kgjEJ_wEpTj2",
        "outputId": "cad4b637-8932-441d-f3ca-f7d7bb894916"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of the first input for layer 1 after dot product: (40000, 5)\n",
            "Shape of the output of layer 1: (40000, 5)\n",
            "Shape of the hidden l1 input with biases: (40000, 6)\n",
            "Shape of the hidden l1 (overall l2) input weighted sum: (40000, 1)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JypRVW74wkhF"
      },
      "execution_count": 28,
      "outputs": []
    }
  ]
}